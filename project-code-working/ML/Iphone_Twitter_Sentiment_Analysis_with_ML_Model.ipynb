{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 2510329,
          "sourceType": "datasetVersion",
          "datasetId": 1520310
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Entity Sentiment Analysis  with Random Forest\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "The goal is to determine the sentiment (Positive, Negative, or Neutral) expressed towards a specific *entity* within a given *tweet*. The dataset treats 'Irrelevant' tweets (those not related to the entity) as 'Neutral'. We will use a pre-trained transformer model, specifically  fine-tuned on the provided training data. The evaluation metric for the competition is Top 1 Classification Accuracy.\n",
        "\n",
        "**Workflow:**\n",
        "\n",
        "1.  **Setup:** Import libraries and define constants (model name, paths, hyperparameters).\n",
        "2.  **Data Loading & Preprocessing:** Load training and validation data, handle missing values, map sentiment labels ('Irrelevant' to 'Neutral', then string labels to integers).\n",
        "3.  **Model & Tokenizer Initialization:** Load the BERTweet model and tokenizer. Includes logic to download from Hugging Face Hub if internet is available, or load from a local save directory otherwise (useful for Kaggle environments).\n",
        "4.  **Dataset Preparation:** Create a custom PyTorch `Dataset` class to format the input (Tweet + Entity) for the BERTweet model. Create `DataLoader` instances for batching and shuffling.\n",
        "5.  **Training Setup:** Define the optimizer (AdamW) and a learning rate scheduler with warmup.\n",
        "6.  **Training & Evaluation Functions:** Define functions for a single training epoch (`train_epoch`) and for evaluating the model (`eval_model`) on the validation set.\n",
        "7.  **Training Loop:** Run the main training loop for a fixed number of epochs, evaluating the model after each epoch and saving the best performing checkpoint based on validation accuracy.\n",
        "8.  **Conclusion:** Print the best validation accuracy achieved."
      ],
      "metadata": {
        "id": "aG7lk6SQF1wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup and Configuration\n",
        "\n",
        "First, we import the necessary libraries and define key configuration parameters for our experiment.\n",
        "\n",
        "*   `pandas`, `numpy`: For data manipulation.\n",
        "*   `os`, `shutil`, `socket`: For interacting with the file system and checking network connectivity.\n",
        "*   `torch`: The core PyTorch library.\n",
        "*   `transformers`: Hugging Face library for accessing pre-trained models (BERTweet), tokenizers, and utility functions like optimizers and schedulers.\n",
        "*   `sklearn.metrics`: For calculating accuracy.\n",
        "*   `tqdm`: For displaying progress bars during training and evaluation."
      ],
      "metadata": {
        "id": "Oqm6bBuWF1wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "import socket\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "trusted": true,
        "id": "5Vrj_r_gF1wR"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration Parameters\n",
        "\n",
        "We define constants for model name, data paths, save directory, and hyperparameters.\n",
        "\n",
        "*   `MODEL_NAME`: We use `bertweet-base`, a BERT model pre-trained specifically on English Tweets, making it suitable for this task.\n",
        "*   `TRAIN_PATH`, `VALID_PATH`: Paths to the competition's training and validation CSV files.\n",
        "*   `SAVE_DIR`: Directory where the fine-tuned model and tokenizer will be saved.\n",
        "*   `MAX_LEN`: Maximum sequence length for tokenization. Tweets longer than this will be truncated.\n",
        "*   `BATCH_SIZE`: Number of samples processed in each training step. Adjust based on GPU memory.\n",
        "*   `EPOCHS`: Number of times to iterate over the entire training dataset.\n",
        "*   `LEARNING_RATE`: The learning rate for the AdamW optimizer."
      ],
      "metadata": {
        "id": "H3bI6DX4F1wR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Loading and Preprocessing\n",
        "\n",
        "We load the training and validation datasets using pandas. The competition dataset doesn't have headers, so we provide column names.\n",
        "\n",
        "**Preprocessing Steps:**\n",
        "\n",
        "1.  **Load Data:** Read CSV files into pandas DataFrames.\n",
        "2.  **Map 'Irrelevant' to 'Neutral':** As per the competition description, we merge the 'Irrelevant' class into the 'Neutral' class.\n",
        "3.  **Handle Missing Values:** Drop rows where the 'Tweet' or 'Sentiment' column is missing.\n",
        "4.  **Create Numerical Labels:** Map the string sentiment labels ('Positive', 'Negative', 'Neutral') to integers (0, 1, 2) for model training.\n",
        "5.  **Verify Labels:** Check for any unexpected sentiment values after mapping and handle potential NaNs in the 'label' column if necessary."
      ],
      "metadata": {
        "id": "HtegLKUmF1wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import pandas as pd\n",
        "# Constructing the direct download link\n",
        "file_id = '1-FqkfXCQ2_O7zSkODq3TVTajNyzUJiG0'\n",
        "direct_download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# Use pandas to read the CSV directly from the URL\n",
        "try:\n",
        "    df = pd.read_csv(direct_download_url)\n",
        "    print(\"File loaded successfully!\")\n",
        "    print(df.head())\n",
        "    train_df = df.copy()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading file: {e}\")\n",
        "    print(\"Please ensure the file is shareable with 'Anyone with the link' and the ID is correct.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwhvuCKxLkYf",
        "outputId": "5f32eaa4-13e6-41c3-85bb-40a178d223e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File loaded successfully!\n",
            "   2401  Borderlands  Positive  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "  im getting on borderlands and i will murder you all ,  \n",
            "0  I am coming to the borders and I will kill you...     \n",
            "1  im getting on borderlands and i will kill you ...     \n",
            "2  im coming on borderlands and i will murder you...     \n",
            "3  im getting on borderlands 2 and i will murder ...     \n",
            "4  im getting into borderlands and i can murder y...     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_df\n",
        "train_df.describe()\n",
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA2H58yqNjNE",
        "outputId": "a7cd5fa0-bb42-4f83-8e1d-0a6e59c17ef5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['2401', 'Borderlands', 'Positive',\n",
              "       'im getting on borderlands and i will murder you all ,'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  add header for the above dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# But in this case, assuming the CSV has a header row:\n",
        "print(\"DataFrame head with existing header:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# You can also inspect the column names directly\n",
        "print(\"\\nDataFrame columns:\")\n",
        "print(train_df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyZjQp8K8D6p",
        "outputId": "38565cf2-909f-47e2-d0f5-b03108f08c08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame head with existing header:\n",
            "   2401  Borderlands  Positive  \\\n",
            "0  2401  Borderlands  Positive   \n",
            "1  2401  Borderlands  Positive   \n",
            "2  2401  Borderlands  Positive   \n",
            "3  2401  Borderlands  Positive   \n",
            "4  2401  Borderlands  Positive   \n",
            "\n",
            "  im getting on borderlands and i will murder you all ,  \n",
            "0  I am coming to the borders and I will kill you...     \n",
            "1  im getting on borderlands and i will kill you ...     \n",
            "2  im coming on borderlands and i will murder you...     \n",
            "3  im getting on borderlands 2 and i will murder ...     \n",
            "4  im getting into borderlands and i can murder y...     \n",
            "\n",
            "DataFrame columns:\n",
            "['2401', 'Borderlands', 'Positive', 'im getting on borderlands and i will murder you all ,']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "TRAIN_PATH = '1-FqkfXCQ2_O7zSkODq3TVTajNyzUJiG0'\n",
        "direct_download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "VALID_PATH='1-FqkfXCQ2_O7zSkODq3TVTajNyzUJiG0'\n",
        "\n",
        "# Constructing the direct download link\n",
        "file_id = '1-FqkfXCQ2_O7zSkODq3TVTajNyzUJiG0'\n",
        "direct_download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# Define column names based on the dataset structure\n",
        "COLUMN_NAMES = ['Tweet ID', 'Entity', 'Sentiment', 'TweetContent']\n",
        "\n",
        "try:\n",
        "\n",
        "    train_df = pd.read_csv(direct_download_url,header=None, names=COLUMN_NAMES)\n",
        "    print(\"Training data loaded successfully!\")\n",
        "    print(train_df.head())\n",
        "    print(\"\\nTraining data info:\")\n",
        "    train_df.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Training data file not found at {TRAIN_PATH}\")\n",
        "    train_df = pd.DataFrame(columns=COLUMN_NAMES) # Create empty DataFrame to avoid errors later\n",
        "\n",
        "# Load validation data\n",
        "try:\n",
        "    #valid_df = pd.read_csv(VALID_PATH, header=None, names=COLUMN_NAMES)\n",
        "    valid_df = pd.read_csv(direct_download_url,header=None, names=COLUMN_NAMES)\n",
        "    print(\"\\nValidation data loaded successfully!\")\n",
        "    print(valid_df.head())\n",
        "    print(\"\\nValidation data info:\")\n",
        "    valid_df.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Validation data file not found at {VALID_PATH}\")\n",
        "    valid_df = pd.DataFrame(columns=COLUMN_NAMES) # Create empty DataFrame to avoid errors later\n",
        "\n",
        "\n",
        "# Map 'Irrelevant' to 'Neutral' and handle missing values\n",
        "if not train_df.empty:\n",
        "    train_df['Sentiment'] = train_df['Sentiment'].replace('Irrelevant', 'Neutral')\n",
        "    train_df.dropna(subset=['TweetContent', 'Sentiment'], inplace=True)\n",
        "    print(\"\\nTraining data after preprocessing:\")\n",
        "    print(train_df['Sentiment'].value_counts())\n",
        "    print(f\"Training data shape after dropna: {train_df.shape}\")\n",
        "\n",
        "if not valid_df.empty:\n",
        "    valid_df['Sentiment'] = valid_df['Sentiment'].replace('Irrelevant', 'Neutral')\n",
        "    valid_df.dropna(subset=['TweetContent', 'Sentiment'], inplace=True)\n",
        "    print(\"\\nValidation data after preprocessing:\")\n",
        "    print(valid_df['Sentiment'].value_counts())\n",
        "    print(f\"Validation data shape after dropna: {valid_df.shape}\")\n",
        "\n",
        "\n",
        "# Create numerical labels\n",
        "SENTIMENT_MAP = {'Positive': 0, 'Negative': 1, 'Neutral': 2}\n",
        "if not train_df.empty:\n",
        "    train_df['label'] = train_df['Sentiment'].map(SENTIMENT_MAP)\n",
        "    # Verify labels - check for any NaNs that might result from unexpected values\n",
        "    if train_df['label'].isnull().any():\n",
        "        print(\"Warning: Found rows with unexpected sentiment values in training data after mapping.\")\n",
        "        print(train_df[train_df['label'].isnull()]['Sentiment'].unique())\n",
        "        # Optionally drop these rows or investigate\n",
        "        train_df.dropna(subset=['label'], inplace=True)\n",
        "        print(f\"Training data shape after dropping rows with invalid labels: {train_df.shape}\")\n",
        "\n",
        "\n",
        "if not valid_df.empty:\n",
        "    valid_df['label'] = valid_df['Sentiment'].map(SENTIMENT_MAP)\n",
        "    # Verify labels - check for any NaNs that might result from unexpected values\n",
        "    if valid_df['label'].isnull().any():\n",
        "        print(\"Warning: Found rows with unexpected sentiment values in validation data after mapping.\")\n",
        "        print(valid_df[valid_df['label'].isnull()]['Sentiment'].unique())\n",
        "        # Optionally drop these rows or investigate\n",
        "        valid_df.dropna(subset=['label'], inplace=True)\n",
        "        print(f\"Validation data shape after dropping rows with invalid labels: {valid_df.shape}\")\n",
        "\n",
        "\n",
        "print(\"\\nData Loading and Preprocessing Complete.\")\n",
        "if not train_df.empty:\n",
        "  print(\"Sample training data with labels:\")\n",
        "  print(train_df[['TweetContent', 'Sentiment', 'label']].head())\n",
        "if not valid_df.empty:\n",
        "  print(\"\\nSample validation data with labels:\")\n",
        "  print(valid_df[['TweetContent', 'Sentiment', 'label']].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YCtTwYzH-Px",
        "outputId": "2338c7f3-d692-4e48-bfb7-0d29c794fc8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data loaded successfully!\n",
            "   Tweet ID       Entity Sentiment  \\\n",
            "0      2401  Borderlands  Positive   \n",
            "1      2401  Borderlands  Positive   \n",
            "2      2401  Borderlands  Positive   \n",
            "3      2401  Borderlands  Positive   \n",
            "4      2401  Borderlands  Positive   \n",
            "\n",
            "                                        TweetContent  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "\n",
            "Training data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74682 entries, 0 to 74681\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Tweet ID      74682 non-null  int64 \n",
            " 1   Entity        74682 non-null  object\n",
            " 2   Sentiment     74682 non-null  object\n",
            " 3   TweetContent  73996 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 2.3+ MB\n",
            "\n",
            "Validation data loaded successfully!\n",
            "   Tweet ID       Entity Sentiment  \\\n",
            "0      2401  Borderlands  Positive   \n",
            "1      2401  Borderlands  Positive   \n",
            "2      2401  Borderlands  Positive   \n",
            "3      2401  Borderlands  Positive   \n",
            "4      2401  Borderlands  Positive   \n",
            "\n",
            "                                        TweetContent  \n",
            "0  im getting on borderlands and i will murder yo...  \n",
            "1  I am coming to the borders and I will kill you...  \n",
            "2  im getting on borderlands and i will kill you ...  \n",
            "3  im coming on borderlands and i will murder you...  \n",
            "4  im getting on borderlands 2 and i will murder ...  \n",
            "\n",
            "Validation data info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74682 entries, 0 to 74681\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Tweet ID      74682 non-null  int64 \n",
            " 1   Entity        74682 non-null  object\n",
            " 2   Sentiment     74682 non-null  object\n",
            " 3   TweetContent  73996 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 2.3+ MB\n",
            "\n",
            "Training data after preprocessing:\n",
            "Sentiment\n",
            "Neutral     30983\n",
            "Negative    22358\n",
            "Positive    20655\n",
            "Name: count, dtype: int64\n",
            "Training data shape after dropna: (73996, 4)\n",
            "\n",
            "Validation data after preprocessing:\n",
            "Sentiment\n",
            "Neutral     30983\n",
            "Negative    22358\n",
            "Positive    20655\n",
            "Name: count, dtype: int64\n",
            "Validation data shape after dropna: (73996, 4)\n",
            "\n",
            "Data Loading and Preprocessing Complete.\n",
            "Sample training data with labels:\n",
            "                                        TweetContent Sentiment  label\n",
            "0  im getting on borderlands and i will murder yo...  Positive      0\n",
            "1  I am coming to the borders and I will kill you...  Positive      0\n",
            "2  im getting on borderlands and i will kill you ...  Positive      0\n",
            "3  im coming on borderlands and i will murder you...  Positive      0\n",
            "4  im getting on borderlands 2 and i will murder ...  Positive      0\n",
            "\n",
            "Sample validation data with labels:\n",
            "                                        TweetContent Sentiment  label\n",
            "0  im getting on borderlands and i will murder yo...  Positive      0\n",
            "1  I am coming to the borders and I will kill you...  Positive      0\n",
            "2  im getting on borderlands and i will kill you ...  Positive      0\n",
            "3  im coming on borderlands and i will murder you...  Positive      0\n",
            "4  im getting on borderlands 2 and i will murder ...  Positive      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face = 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"
      ],
      "metadata": {
        "id": "BGf4BPn-SoW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#perfrom random forest analysis\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming train_df and valid_df are already loaded and preprocessed as in the preceding code\n",
        "\n",
        "if train_df.empty or valid_df.empty:\n",
        "    print(\"Training or validation data is empty. Cannot perform Random Forest analysis.\")\n",
        "else:\n",
        "    # Combine training and validation data for TF-IDF vectorization\n",
        "    all_tweets = pd.concat([train_df['TweetContent'], valid_df['TweetContent']], axis=0)\n",
        "\n",
        "    # Create TF-IDF features\n",
        "    # Consider using bigrams or trigrams for better representation\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "    X_combined = vectorizer.fit_transform(all_tweets)\n",
        "\n",
        "    # Split features back into training and validation sets\n",
        "    X_train = X_combined[:len(train_df)]\n",
        "    X_valid = X_combined[len(train_df):]\n",
        "\n",
        "    y_train = train_df['label']\n",
        "    y_valid = valid_df['label']\n",
        "\n",
        "    # Initialize and train the Random Forest classifier\n",
        "    # You can tune n_estimators, max_depth, etc.\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = rf_classifier.predict(X_valid)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_valid, y_pred)\n",
        "    print(f\"\\nRandom Forest Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Optional: Display a classification report\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(\"\\nRandom Forest Classification Report:\")\n",
        "    print(classification_report(y_valid, y_pred, target_names=SENTIMENT_MAP.keys()))\n"
      ],
      "metadata": {
        "id": "bRZjj4ix7VME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# If you have the actual predictions (y_pred) and true labels (y_valid)\n",
        "if 'y_valid' in locals() and 'y_pred' in locals():\n",
        "    # Visualize the confusion matrix\n",
        "    cm = confusion_matrix(y_valid, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=SENTIMENT_MAP.keys(), yticklabels=SENTIMENT_MAP.keys())\n",
        "    plt.title('Confusion Matrix for Random Forest Model')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot generate visualizations. Validation DataFrame or predictions are not available.\")\n"
      ],
      "metadata": {
        "id": "RaJLaL5-7bmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Iphone Entity Sentiment Analysis with Random Forest\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "The goal of this project is to determine the sentiment (Positive, Negative, or Neutral) expressed toward a specific entity mentioned in an iPhone review dataset.\n",
        "\n",
        "Unlike deep learning transformer-based models, we utilize a Random Forest classifier—a traditional machine learning approach. This enables efficient training on modest computational resources while maintaining solid performance in textual classification tasks.\n"
      ],
      "metadata": {
        "id": "C3ZZ3iYRCJMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset titled Iphone.csv consists of 3,062 rows representing customer reviews for various iPhone models sold primarily in India. It includes detailed metadata for each review such as the productAsin (a unique product identifier), the country of the review, the date it was posted, and a boolean flag isVerified indicating whether the purchase was verified. One of the central features is ratingScore, a numerical value from 1 to 5 reflecting the customer’s satisfaction. Accompanying this rating are two textual fields—reviewTitle and reviewDescription—which provide qualitative feedback and are essential for natural language processing tasks like sentiment analysis.\n",
        "\n",
        "Each review entry also contains a reviewUrl linking to the full review on Amazon and a reviewedIn field that combines date and country in natural language format. Furthermore, the dataset captures product variation details in the variant and variantAsin columns, specifying color and storage options. The reviewDescription column, which contains the core text of the user’s feedback, is key for training and evaluating sentiment models. However, some entries in this column are missing (about 2.8%), and they must be handled during preprocessing. Overall, the dataset is well-suited for supervised learning, especially for classifying review sentiments based on star ratings or predicting user feedback using natural language techniques."
      ],
      "metadata": {
        "id": "bYCAeoXOA3iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# iPhone Review Sentiment Analysis Workflow (Random Forest Classifier)\n",
        "\n",
        "This document outlines the complete workflow to process the `iphone (1).csv` dataset, apply sentiment labeling, and train a machine learning model to predict sentiment based on user reviews.\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 1: Data Loading & Exploration\n",
        "- Load the dataset using `pandas`.\n",
        "- Examine columns, data types, and null values.\n",
        "- Focus on two fields:\n",
        "  - `reviewDescription`: The review text (input).\n",
        "  - `ratingScore`: Used to derive sentiment (target).\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 2: Sentiment Labeling\n",
        "Convert `ratingScore` into sentiment categories:\n",
        "- `1` or `2` → **Negative**\n",
        "- `3` → **Neutral**\n",
        "- `4` or `5` → **Positive**\n",
        "\n",
        "Drop entries where `reviewDescription` is missing.\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 3: Text Preprocessing\n",
        "- Convert text to lowercase.\n",
        "- Remove punctuation, special characters, and numbers.\n",
        "- Remove stopwords (e.g., \"the\", \"and\", \"is\").\n",
        "- (Optional) Apply stemming or lemmatization to normalize words.\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 4: Feature Engineering\n",
        "- Use `CountVectorizer` to convert text to numerical vectors (Bag-of-Words).\n",
        "- Consider n-grams (bigrams or trigrams) for richer context.\n",
        "- Optionally use `TfidfVectorizer` for better word importance scaling.\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 5: Model Training\n",
        "- Split the data into training and test sets.\n",
        "- Train a **Random Forest Classifier** using the feature matrix.\n",
        "- Use cross-validation for robustness.\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 6: Model Evaluation\n",
        "- Predict sentiments on the test data.\n",
        "- Evaluate using:\n",
        "  - Accuracy\n",
        "  - Precision, Recall, F1-score\n",
        "  - Confusion Matrix\n",
        "- Visualize performance using seaborn/matplotlib.\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 7: Interpretation\n",
        "- Display top contributing words (features) from the Random Forest model.\n",
        "- Analyze:\n",
        "  - Class imbalance (e.g., more positive reviews than negative)\n",
        "  - Error patterns (e.g., confusion between neutral and positive)\n",
        "\n",
        "---\n",
        "\n",
        "##  Step 8: Deployment\n",
        "- Wrap the model into a FastAPI or MCP tool for API access.\n",
        "- Use with Claude UI for query-based summarization or insights.\n",
        "\n",
        "---\n",
        "\n",
        "This workflow combines classical machine learning with light text processing and allows fast evaluation of user sentiment toward iPhone products.\n"
      ],
      "metadata": {
        "id": "vS476y6YArvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load Data\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/aai-510/project/Iphone.csv')\n",
        "df = df.dropna(subset=['reviewDescription', 'ratingScore']).fillna('Unknown')\n",
        "df['isGood'] = (df['ratingScore'] >= 4).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwAI2dMRkWAN",
        "outputId": "40e83034-0d6e-40c3-9dae-2dd0696d91be"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install nltk transformers torch\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Corrected resource name\n",
        "\n",
        "# Initialize Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK POS tags to WordNet POS tags for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Maps NLTK POS tags to WordNet tags.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Function to tokenize and lemmatize text\n",
        "def tokenize_and_lemmatize(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in tokens]\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Apply tokenization and lemmatization\n",
        "df['processed_review'] = df['reviewDescription'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# Create TF-IDF Embeddings\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # You can adjust max_features\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_review'])\n",
        "\n",
        "print(\"Original reviews:\")\n",
        "print(df['reviewDescription'].head())\n",
        "\n",
        "print(\"\\nProcessed (Tokenized and Lemmatized) reviews:\")\n",
        "print(df['processed_review'].head())\n",
        "\n",
        "print(\"\\nShape of TF-IDF matrix (Number of reviews, Number of features):\")\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "# Display a small portion of the TF-IDF matrix and feature names\n",
        "print(\"\\nSample TF-IDF values:\")\n",
        "print(tfidf_matrix[:2].toarray())\n",
        "\n",
        "print(\"\\nSample TF-IDF feature names (vocabulary):\")\n",
        "print(tfidf_vectorizer.get_feature_names_out()[:20])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21f2ci-D3YFn",
        "outputId": "9e4596ca-116c-47eb-8138-5b0384274015"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original reviews:\n",
            "0    Every thing is good about iPhones, there's not...\n",
            "1    It look so fabulous, I am android user switche...\n",
            "2    I tried to flip camera while recording but no ...\n",
            "3                                         100% genuine\n",
            "4           Happy to get the iPhone 13 in Amazon offer\n",
            "Name: reviewDescription, dtype: object\n",
            "\n",
            "Processed (Tokenized and Lemmatized) reviews:\n",
            "0    Every thing be good about iPhones , there 's n...\n",
            "1    It look so fabulous , I be android user switch...\n",
            "2    I try to flip camera while record but no facil...\n",
            "3                                        100 % genuine\n",
            "4           Happy to get the iPhone 13 in Amazon offer\n",
            "Name: processed_review, dtype: object\n",
            "\n",
            "Shape of TF-IDF matrix (Number of reviews, Number of features):\n",
            "(2976, 5000)\n",
            "\n",
            "Sample TF-IDF values:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "Sample TF-IDF feature names (vocabulary):\n",
            "['00' '000' '02' '03' '05' '06' '09' '0display' '10' '100' '1000'\n",
            " '100percent' '10battery' '10camera' '10display' '10k' '10no' '10sound'\n",
            " '10video' '11']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Prepare data for Random Forest\n",
        "X = tfidf_matrix  # Features from TF-IDF\n",
        "y = df['isGood']  # Target variable (0 for bad, 1 for good)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Stratify helps maintain proportion of good/bad reviews\n",
        "\n",
        "print(\"\\nShape of training features:\", X_train.shape)\n",
        "print(\"Shape of testing features:\", X_test.shape)\n",
        "print(\"Shape of training labels:\", y_train.shape)\n",
        "print(\"Shape of testing labels:\", y_test.shape)\n",
        "\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight='balanced') # Added class_weight='balanced' to handle potential class imbalance\n",
        "\n",
        "print(\"\\nTraining Random Forest model...\")\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"Random Forest model training complete.\")\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nRandom Forest Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print classification report for more detailed evaluation\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Bad Review', 'Good Review']))\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Bad Review', 'Good Review'], yticklabels=['Bad Review', 'Good Review'])\n",
        "plt.title('Confusion Matrix for Random Forest Model')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lO2nm3G87xKz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}